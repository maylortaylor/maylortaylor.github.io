[
  {
    "id": "d4-001",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "acronym",
    "front": "SIEM",
    "back": {
      "level1": "Security Information and Event Management",
      "level2": "A centralized system that aggregates, correlates, and analyzes security event data from across an organization's infrastructure. SIEM solutions collect logs from firewalls, IDS/IPS, servers, and applications to provide real-time analysis and alerting.",
      "level3": "Exam tip: SIEM is often tested with questions about log aggregation, correlation rules, and incident detection. Remember that SIEM combines both SIM (Security Information Management) and SEM (Security Event Management). Common vendors include Splunk, IBM QRadar, and ArcSight. Know that SIEM can help with compliance reporting and forensic analysis."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SOAR", "IDS", "IPS", "Log Aggregation"]
    }
  },
  {
    "id": "d4-002",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "acronym",
    "front": "SOAR",
    "back": {
      "level1": "Security Orchestration, Automation, and Response",
      "level2": "A platform that integrates security tools and automates incident response workflows. SOAR solutions enable security teams to define playbooks that automatically respond to common security events, reducing manual intervention and response times.",
      "level3": "Exam tip: SOAR differs from SIEM by focusing on automation and orchestration of response actions. While SIEM detects and alerts, SOAR takes action. Know that SOAR can integrate with multiple security tools (firewalls, EDR, ticketing systems) and execute automated remediation. Common use cases include automated malware containment, user account lockouts, and threat intelligence enrichment."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SIEM", "Playbook", "Automation", "Orchestration"]
    }
  },
  {
    "id": "d4-003",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "tool",
    "front": "IDS",
    "back": {
      "level1": "Intrusion Detection System",
      "level2": "A passive monitoring system that analyzes network traffic or host activity to detect potential security threats. IDS can be network-based (NIDS) or host-based (HIDS). When suspicious activity is detected, it generates alerts but does not take action to block the traffic.",
      "level3": "Exam tip: Key distinction - IDS DETECTS but does NOT block (unlike IPS). Know the two detection methods: signature-based (matches known attack patterns) and anomaly-based (detects deviations from baseline behavior). Common IDS tools include Snort and Suricata. Remember that IDS generates false positives and requires tuning. Placement is typically at network boundaries or critical segments."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["IPS", "NIDS", "HIDS", "Snort"]
    }
  },
  {
    "id": "d4-004",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "tool",
    "front": "IPS",
    "back": {
      "level1": "Intrusion Prevention System",
      "level2": "An active security device that monitors network traffic and can take automatic action to block or prevent detected threats. IPS sits inline with network traffic and can drop malicious packets, reset connections, or block IP addresses in real-time.",
      "level3": "Exam tip: Key distinction - IPS both DETECTS and BLOCKS threats (unlike IDS which only detects). Must be placed inline with traffic, which means it can impact network performance if not properly sized. Know the risk of false positives causing legitimate traffic to be blocked. IPS can use signature-based, anomaly-based, and policy-based detection. Common to see questions comparing IDS vs IPS placement and capabilities."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["IDS", "NIPS", "HIPS", "Inline"]
    }
  },
  {
    "id": "d4-005",
    "domain": 4,
    "section": "Incident Response",
    "type": "process",
    "front": "NIST IR Lifecycle",
    "back": {
      "level1": "Preparation → Detection & Analysis → Containment, Eradication & Recovery → Post-Incident Activity",
      "level2": "NIST SP 800-61 defines four phases of incident response: (1) Preparation - establishing IR capabilities, (2) Detection & Analysis - identifying and analyzing incidents, (3) Containment, Eradication & Recovery - limiting damage and restoring systems, (4) Post-Incident Activity - lessons learned and improvement.",
      "level3": "Exam tip: This is heavily tested. Memorize the four phases in order. Know that Preparation includes training, tools, and communication plans. Detection & Analysis involves triage and severity assessment. Containment has short-term (isolate infected systems) and long-term (patch vulnerabilities) aspects. Post-incident includes root cause analysis and updating response procedures. Common scenario questions test which phase an activity belongs to."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["NIST", "SP 800-61", "Incident Response", "IR"]
    }
  },
  {
    "id": "d4-006",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Containment Strategies",
    "back": {
      "level1": "Short-term containment and Long-term containment",
      "level2": "Short-term containment focuses on immediate isolation to prevent spread (disconnect from network, disable accounts). Long-term containment involves applying temporary fixes while preparing for full eradication (patching, system rebuilding). The strategy depends on the incident type, business impact, and available resources.",
      "level3": "Exam tip: Know the difference between the two types. Short-term is quick and may impact availability (example: pulling network cable). Long-term allows business continuity while preparing for recovery (example: applying temporary patches, moving to segmented VLAN). Questions often ask which containment method is appropriate for specific scenarios like ransomware (aggressive short-term) vs. data exfiltration (careful long-term to preserve evidence)."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Isolation", "Eradication"]
    }
  },
  {
    "id": "d4-007",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Order of Volatility",
    "back": {
      "level1": "CPU cache/registers → RAM → Swap/pagefile → Hard disk → Remote logs → Physical configuration",
      "level2": "The order in which digital evidence should be collected, from most volatile (lost when power is removed) to least volatile (persistent). Following this order ensures that the most fragile evidence is captured first before it is lost or altered.",
      "level3": "Exam tip: Memorize this order as it's frequently tested. Start with CPU registers and cache (most volatile), then system memory (RAM), then temporary file systems, then hard drives, then remote logs and backups, finally physical configuration and topology. Know that live system analysis must capture volatile data before powering down. Common question: what should be collected first in an investigation? Always choose the most volatile option."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Evidence Collection", "Volatile Data"]
    }
  },
  {
    "id": "d4-008",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Chain of Custody",
    "back": {
      "level1": "Documentation tracking who handled evidence, when, where, and why",
      "level2": "A chronological paper trail documenting the collection, transfer, analysis, and storage of evidence. It must include: who collected it, date/time, how it was collected, who has accessed it, and where it has been stored. This ensures evidence integrity and admissibility in legal proceedings.",
      "level3": "Exam tip: Chain of custody is critical for legal proceedings. Any break in the chain can render evidence inadmissible in court. Know that it requires: proper documentation, secure storage with access logs, hash values to prove integrity, and witness signatures. Common mistakes include failure to document transfers, allowing unauthorized access, or improper storage. Questions often test scenarios where chain of custody is broken or maintained."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Evidence", "Legal Hold", "Documentation"]
    }
  },
  {
    "id": "d4-009",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "tool",
    "front": "Write Blocker",
    "back": {
      "level1": "A device that allows read-only access to storage media during forensic acquisition",
      "level2": "Hardware or software that prevents any modification to evidence drives during imaging or analysis. It allows investigators to create forensic copies while ensuring the original evidence remains unaltered, maintaining integrity and admissibility.",
      "level3": "Exam tip: Write blockers are essential in forensics to prevent accidental modification of evidence. Know that hardware write blockers are preferred over software because they're more reliable. They work by intercepting write commands at the hardware level. Used during disk imaging with tools like FTK Imager or dd. Remember that even mounting a drive without a write blocker can modify access timestamps, potentially compromising evidence."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Evidence Preservation", "Disk Imaging"]
    }
  },
  {
    "id": "d4-010",
    "domain": 4,
    "section": "Security Automation",
    "type": "acronym",
    "front": "SCAP",
    "back": {
      "level1": "Security Content Automation Protocol",
      "level2": "A suite of specifications maintained by NIST that standardizes the format for security automation data. SCAP enables automated vulnerability scanning, configuration compliance checking, and security measurement using standardized languages like OVAL, XCCDF, and CVE.",
      "level3": "Exam tip: Know that SCAP is not a single tool but a framework of multiple specifications. Key components include: CVE (Common Vulnerabilities and Exposures), OVAL (Open Vulnerability Assessment Language), XCCDF (Extensible Configuration Checklist Description Format), and CPE (Common Platform Enumeration). Used by tools like Nessus and SCAP scanners. Common in government compliance requirements. Questions may ask about which component handles specific tasks."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["NIST", "Automation", "Vulnerability Scanning", "CVE", "OVAL"]
    }
  },
  {
    "id": "d4-011",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "acronym",
    "front": "CVE",
    "back": {
      "level1": "Common Vulnerabilities and Exposures",
      "level2": "A public database of known cybersecurity vulnerabilities maintained by MITRE. Each vulnerability is assigned a unique CVE identifier (e.g., CVE-2021-44228 for Log4Shell) that allows security professionals to reference and track specific vulnerabilities across different platforms and tools.",
      "level3": "Exam tip: CVE identifiers follow the format CVE-YEAR-NUMBER. Know that CVE provides standardized names but doesn't include severity scores (that's CVSS). The CVE database is used by vulnerability scanners, patch management systems, and security advisories. Understand the difference: CVE identifies the vulnerability, CVSS scores its severity, and vendor patches fix it. Common to see questions about the CVE numbering system or what CVE provides."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["CVSS", "Vulnerability", "MITRE", "Patch Management"]
    }
  },
  {
    "id": "d4-012",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "acronym",
    "front": "CVSS",
    "back": {
      "level1": "Common Vulnerability Scoring System",
      "level2": "An open framework for communicating the severity of software vulnerabilities using a numerical score from 0-10. CVSS scores are calculated based on metrics like exploitability, impact, and complexity. Scores are categorized as Low (0.1-3.9), Medium (4.0-6.9), High (7.0-8.9), or Critical (9.0-10.0).",
      "level3": "Exam tip: Memorize the score ranges and severity levels. CVSS v3.1 is the current version. Know the three metric groups: Base (intrinsic vulnerability characteristics), Temporal (changes over time like exploit availability), and Environmental (organization-specific factors). Base score is most commonly used. Questions often test which severity level corresponds to a given score, or ask about prioritizing patches based on CVSS scores."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["CVE", "Vulnerability", "Severity", "Risk Assessment"]
    }
  },
  {
    "id": "d4-013",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Vulnerability Scanning Types",
    "back": {
      "level1": "Credentialed (authenticated) vs. Non-credentialed (unauthenticated) scanning",
      "level2": "Credentialed scanning uses valid login credentials to access systems and perform deeper inspection of configurations, installed software, and patch levels. Non-credentialed scanning examines systems from the network perspective without logging in, similar to how an external attacker would see the system.",
      "level3": "Exam tip: Know the trade-offs. Credentialed scans provide more accurate and detailed results (can see missing patches, misconfigurations) but require credential management and trust. Non-credentialed scans show the external attack surface but may miss internal vulnerabilities or produce more false positives. Questions often ask which type is appropriate for specific scenarios: use credentialed for internal compliance checks, non-credentialed for external penetration testing perspective."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Vulnerability Scanning", "Nessus", "Authentication"]
    }
  },
  {
    "id": "d4-014",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "acronym",
    "front": "EDR",
    "back": {
      "level1": "Endpoint Detection and Response",
      "level2": "A security solution that continuously monitors and collects data from endpoints (workstations, servers, mobile devices) to detect and respond to threats. EDR provides visibility into endpoint activities, behavioral analysis, and automated or manual response capabilities like isolating infected systems.",
      "level3": "Exam tip: EDR is evolution of traditional antivirus. Key capabilities include: continuous monitoring, behavioral analysis (not just signatures), threat hunting, forensic investigation, and automated response. Differs from antivirus by providing visibility and context, not just blocking. Common vendors: CrowdStrike, Carbon Black, SentinelOne. Know that EDR can detect fileless malware and living-off-the-land attacks that traditional AV misses. Often tested with XDR (Extended Detection and Response)."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["XDR", "Antivirus", "Endpoint Security", "Behavioral Analysis"]
    }
  },
  {
    "id": "d4-015",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "acronym",
    "front": "XDR",
    "back": {
      "level1": "Extended Detection and Response",
      "level2": "An evolution of EDR that integrates security data from multiple sources beyond just endpoints, including networks, cloud, email, and applications. XDR correlates data across these domains to provide holistic threat detection and automated response across the entire environment.",
      "level3": "Exam tip: Think of XDR as EDR++ or 'EDR for everything'. Key difference from EDR: broader scope beyond endpoints. While EDR focuses on workstations/servers, XDR adds network traffic, cloud workloads, email gateways, and identity systems. Benefits include better threat correlation (connect email phish → endpoint compromise → lateral movement) and unified response. Know that XDR reduces tool sprawl and alert fatigue by centralizing security operations."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": false,
      "relatedTerms": ["EDR", "SIEM", "Security Operations", "Correlation"]
    }
  },
  {
    "id": "d4-016",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Incident Severity Levels",
    "back": {
      "level1": "Typically categorized as Critical, High, Medium, Low based on impact and scope",
      "level2": "Organizations classify incidents by severity to prioritize response efforts. Critical incidents cause severe business impact (data breach, ransomware, critical system outage). High incidents have significant impact. Medium incidents have limited impact. Low incidents have minimal impact. Severity determines response time, escalation procedures, and resources allocated.",
      "level3": "Exam tip: Know that severity classification considers: business impact, affected systems, data sensitivity, number of users impacted, and potential for spreading. Critical incidents trigger immediate executive notification and may invoke business continuity plans. Questions often present scenarios and ask you to classify severity. Key factors: Is PII/PHI exposed? Are critical systems down? Can it spread? Is there active data exfiltration? Higher severity = faster response time requirements."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Triage", "Escalation", "Impact Assessment"]
    }
  },
  {
    "id": "d4-017",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Forensic Disk Imaging",
    "back": {
      "level1": "Creating a bit-for-bit copy of storage media for analysis while preserving the original evidence",
      "level2": "The process of creating an exact duplicate of a storage device, capturing every bit including deleted files, slack space, and hidden partitions. Uses tools like dd, FTK Imager, or EnCase with write blockers. The image is hashed (MD5/SHA256) to verify integrity and prove it matches the original.",
      "level3": "Exam tip: Know the imaging process: (1) attach write blocker, (2) connect source and destination drives, (3) use imaging tool to create bit-for-bit copy, (4) generate hash of source, (5) generate hash of image, (6) verify hashes match, (7) document everything. Common formats: raw/dd (exact copy), E01 (Expert Witness Format with compression and metadata), AFF (Advanced Forensic Format). Analysis is done on the IMAGE, never the original. Questions test proper procedure and hash verification."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Write Blocker", "Hash", "dd", "FTK Imager"]
    }
  },
  {
    "id": "d4-018",
    "domain": 4,
    "section": "Security Automation",
    "type": "concept",
    "front": "Playbooks and Runbooks",
    "back": {
      "level1": "Playbooks define automated workflows; Runbooks provide step-by-step procedures",
      "level2": "Playbooks are automated decision trees used by SOAR platforms to respond to security events (if phishing email detected, then isolate mailbox, scan attachments, notify user). Runbooks are detailed manual procedures that guide analysts through response steps (how to investigate a phishing email step-by-step).",
      "level3": "Exam tip: Key distinction - Playbooks are for AUTOMATION (executed by SOAR), Runbooks are for GUIDANCE (followed by humans). Playbooks contain logic flows and integrate with tools. Runbooks contain detailed instructions for analysts. Both are part of incident response preparation. Questions often test which to use: automated, repeatable tasks = playbook; complex decisions requiring human judgment = runbook. Examples: Playbook automates malware containment; Runbook guides analyst through forensic analysis."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SOAR", "Automation", "Incident Response", "Procedures"]
    }
  },
  {
    "id": "d4-019",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "UEBA",
    "back": {
      "level1": "User and Entity Behavior Analytics",
      "level2": "A security technology that uses machine learning and analytics to establish baseline behaviors for users and entities (devices, applications), then detects anomalies that may indicate compromise. UEBA can identify insider threats, compromised credentials, and advanced persistent threats that evade signature-based detection.",
      "level3": "Exam tip: UEBA focuses on BEHAVIOR not signatures. It learns what's normal (user X typically accesses files from location Y during hours Z) and alerts on deviations (user X suddenly downloads massive data at 3 AM from foreign IP). Key use cases: detecting compromised accounts, insider threats, privilege escalation. Often integrated with SIEM. Differs from traditional monitoring by using AI/ML to spot subtle anomalies. Questions test understanding of behavioral vs signature-based detection."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Behavioral Analysis", "Machine Learning", "Anomaly Detection", "Insider Threat"]
    }
  },
  {
    "id": "d4-020",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Patch Management Phases",
    "back": {
      "level1": "Identify → Test → Deploy → Verify",
      "level2": "A systematic approach to applying security updates: (1) Identify available patches from vendors and vulnerability scans, (2) Test patches in non-production environment for compatibility, (3) Deploy patches to production systems in controlled manner, (4) Verify successful installation and system functionality.",
      "level3": "Exam tip: Know that patch management is a continuous cycle. NEVER deploy untested patches directly to production - testing phase is critical to avoid breaking production systems. Deployment often uses staged rollout (pilot group first, then wider deployment). Verification includes confirming patch version and testing system functionality. Emergency patches for actively exploited vulnerabilities may have accelerated timeline but still need testing. Questions test proper sequence and importance of testing phase."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Patch Management", "Change Management", "Vulnerability Management"]
    }
  },
  {
    "id": "d4-021",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "NetFlow",
    "back": {
      "level1": "Network protocol that collects IP traffic flow information for monitoring and analysis",
      "level2": "NetFlow captures metadata about network traffic (source/destination IPs, ports, protocols, bytes transferred, timestamps) without capturing actual packet content. Exported from routers and switches to collectors for analysis. Used for bandwidth monitoring, traffic analysis, and security monitoring.",
      "level3": "Exam tip: NetFlow provides METADATA, not full packet captures. It's efficient for monitoring large networks because it only captures flow records (who talked to whom, when, how much), not packet payloads. Key fields: source/dest IP, source/dest port, protocol, interface, byte count. Variants include sFlow, IPFIX (NetFlow v10), and jFlow. Used to detect: DDoS attacks (abnormal traffic patterns), data exfiltration (large outbound transfers), network reconnaissance (port scanning). Less resource-intensive than full packet capture."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Network Monitoring", "Traffic Analysis", "sFlow", "IPFIX"]
    }
  },
  {
    "id": "d4-022",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Memory Forensics",
    "back": {
      "level1": "Analysis of volatile system memory (RAM) to investigate security incidents",
      "level2": "Capturing and analyzing the contents of RAM to find evidence that may not exist on disk, such as running processes, network connections, encryption keys, passwords, and malware running only in memory. Uses tools like Volatility, Rekall, or memory dump utilities.",
      "level3": "Exam tip: Memory forensics is critical because: (1) malware may run fileless (only in RAM), (2) encryption keys exist in memory, (3) shows current system state at time of capture. Must be captured BEFORE shutting down (RAM is volatile). Tools like Volatility can extract: running processes, network connections, loaded DLLs, registry hives, command history. Common artifacts: process injection, hidden processes, network sockets. Questions test knowing when memory forensics is needed (fileless malware, live incident response) vs disk forensics."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Volatile Data", "Volatility", "RAM Analysis"]
    }
  },
  {
    "id": "d4-023",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Communication Plan",
    "back": {
      "level1": "Documented procedures defining how to communicate during security incidents",
      "level2": "Establishes who needs to be notified, when, through what channels, and what information to share during incidents. Includes internal stakeholders (executives, legal, HR, IT), external parties (customers, regulators, law enforcement, media), and defines escalation paths and templates.",
      "level3": "Exam tip: Communication plans are part of IR Preparation phase. Key elements: contact lists with multiple methods (email, phone, SMS), escalation criteria (when to notify CEO, when to call law enforcement), pre-approved message templates, secure communication channels (out-of-band in case primary is compromised). Know legal requirements: data breach notification laws, regulatory reporting timelines. Common mistakes: using compromised systems to communicate, delaying required notifications, inconsistent messaging. Questions test proper notification timing and channels."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Escalation", "Stakeholders", "Breach Notification"]
    }
  },
  {
    "id": "d4-024",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "False Positives vs False Negatives",
    "back": {
      "level1": "False Positive: alert when no threat exists; False Negative: no alert when threat exists",
      "level2": "False positives occur when security tools flag benign activity as malicious (IDS alerts on legitimate traffic, vulnerability scanner reports patched system as vulnerable). False negatives occur when tools miss actual threats (malware passes through AV, scanner misses real vulnerability).",
      "level3": "Exam tip: Know the risk trade-offs. False positives cause alert fatigue and waste analyst time but are generally LESS DANGEROUS. False negatives allow real attacks to proceed undetected and are MORE DANGEROUS. Tuning security tools involves balancing sensitivity: stricter rules reduce false negatives but increase false positives. Common causes of false positives: outdated signatures, baseline not adjusted for environment. Reducing false positives requires tuning, whitelisting, baseline refinement. Questions test which is worse in given scenarios."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Detection", "Tuning", "IDS", "Vulnerability Scanning"]
    }
  },
  {
    "id": "d4-025",
    "domain": 4,
    "section": "Security Automation",
    "type": "acronym",
    "front": "STIX/TAXII",
    "back": {
      "level1": "Structured Threat Information eXpression / Trusted Automated eXchange of Intelligence Information",
      "level2": "STIX is a standardized language for describing cyber threat information (indicators, tactics, malware). TAXII is a protocol for sharing STIX data between organizations. Together they enable automated threat intelligence sharing in machine-readable format.",
      "level3": "Exam tip: STIX defines WHAT to share (the language/format for threat data), TAXII defines HOW to share it (the transport protocol). Think of it like: STIX is like JSON/XML format, TAXII is like HTTP/HTTPS for transferring it. Used by threat intelligence platforms and sharing communities (ISACs). Benefits: automated consumption of threat feeds, standardized format enables tool integration. Questions test knowing which is format vs protocol, and use cases for threat intelligence sharing."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": false,
      "relatedTerms": ["Threat Intelligence", "Information Sharing", "Automation", "IOC"]
    }
  },
  {
    "id": "d4-026",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Baseline Configuration",
    "back": {
      "level1": "A documented secure state of systems and network used as a reference point",
      "level2": "Establishing a known-good configuration for systems including installed software, security settings, network traffic patterns, and user behavior. Baselines enable detection of deviations that may indicate compromise, misconfiguration, or policy violations. Monitored using configuration management tools and security monitoring.",
      "level3": "Exam tip: Baselines are used for anomaly detection and compliance checking. Types include: security baselines (CIS benchmarks, DISA STIGs), configuration baselines (gold images), performance baselines (normal CPU/memory/network usage), behavioral baselines (typical user activity). Deviations trigger alerts. Know that baselines must be maintained and updated as environment changes. Used by tools like SCAP scanners, UEBA, and configuration management. Questions test identifying when baselines are useful and how to maintain them."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Configuration Management", "Anomaly Detection", "CIS Benchmarks", "Gold Image"]
    }
  },
  {
    "id": "d4-027",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Lessons Learned",
    "back": {
      "level1": "Post-incident review process to improve future response effectiveness",
      "level2": "A structured meeting held after incident resolution where the response team reviews what happened, what worked well, what didn't, and what should be improved. Findings are documented and used to update IR plans, playbooks, training, and technical controls. Part of NIST's Post-Incident Activity phase.",
      "level3": "Exam tip: Lessons learned is the FINAL phase of NIST IR lifecycle. Held within days/weeks after incident while details are fresh. Key questions asked: What happened? When was it detected? How well did response procedures work? What would we do differently? Outputs include: updated IR procedures, new detection rules, identified training needs, recommended technical improvements. Know this is a blameless process focused on improvement. Questions test knowing this is part of post-incident phase and its purpose."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Post-Incident Activity", "NIST", "Continuous Improvement"]
    }
  },
  {
    "id": "d4-028",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Legal Hold",
    "back": {
      "level1": "Process of preserving all relevant data when litigation or investigation is anticipated",
      "level2": "A legal requirement to preserve potentially relevant electronic and physical records when litigation is reasonably anticipated. Organizations must suspend normal document retention/deletion policies and preserve all data that might be relevant. Failure to preserve can result in sanctions or adverse inference.",
      "level3": "Exam tip: Legal hold triggers when litigation or investigation is anticipated, not just when lawsuit is filed. Applies to emails, documents, logs, backups - anything potentially relevant. IT must: suspend auto-deletion policies, preserve backup tapes, prevent users from deleting files, document preservation efforts. Spoliation (destruction of evidence) can result in severe legal penalties. Know that legal hold overrides normal retention policies. Questions test knowing when to initiate and what data to preserve."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "eDiscovery", "Evidence Preservation", "Litigation"]
    }
  },
  {
    "id": "d4-029",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Compensating Controls",
    "back": {
      "level1": "Alternative security measures when primary controls cannot be implemented",
      "level2": "Controls deployed when preferred security measures are not technically or operationally feasible. They provide equivalent or similar protection through different means. Example: if system cannot be patched due to vendor dependency, compensating controls might include network segmentation, IPS signatures, and enhanced monitoring.",
      "level3": "Exam tip: Compensating controls are used when primary control is impractical. Common scenarios: legacy systems that can't be patched, business requirement prevents security control, cost prohibitive. Know that compensating controls must provide EQUIVALENT protection - just being 'something' isn't enough. Examples: Can't encrypt database? Use network encryption + access controls. Can't patch? Use virtual patching via IPS + isolation. Document compensating controls for compliance. Questions test identifying appropriate compensating controls for scenarios."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Risk Mitigation", "Security Controls", "Legacy Systems", "Virtual Patching"]
    }
  },
  {
    "id": "d4-030",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Security Orchestration",
    "back": {
      "level1": "Connecting security tools and data sources to enable automated workflows",
      "level2": "The 'O' in SOAR - integrating disparate security tools (SIEM, firewalls, EDR, ticketing systems, threat intel) into a unified platform that can share data and trigger coordinated actions. Enables automated response workflows that involve multiple systems without manual intervention.",
      "level3": "Exam tip: Orchestration is about INTEGRATION and COORDINATION of tools. Think of it as the conductor of an orchestra - it doesn't play instruments but coordinates them. Example: when SIEM detects suspicious login, orchestration platform can automatically: query EDR for process info, check threat intel feed, update firewall rules, create ticket, notify analyst. Benefits: faster response, consistency, reduced manual work. Differs from automation (which can be single-tool) by coordinating MULTIPLE tools. Questions test understanding orchestration vs automation."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SOAR", "Automation", "Integration", "Workflow"]
    }
  },
  {
    "id": "d4-031",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Cyber Kill Chain",
    "back": {
      "level1": "Reconnaissance → Weaponization → Delivery → Exploitation → Installation → Command & Control → Actions on Objectives",
      "level2": "Lockheed Martin's 7-stage framework describing the phases of a cyber attack. Starts with reconnaissance (gathering information), through weaponization (creating malware), delivery (sending to target), exploitation (triggering vulnerability), installation (persisting on system), C2 (establishing communication), and finally actions on objectives (achieving attacker's goal like data theft).",
      "level3": "Exam tip: MEMORIZE all 7 stages in order. This framework is heavily tested. Know that defensive strategies map to each stage: reconnaissance (threat intelligence), delivery (email filtering), exploitation (patching), installation (antivirus), C2 (firewall/IDS), actions (DLP). Questions often ask which stage an activity belongs to or which defense stops which stage. Key insight: breaking the chain at ANY point stops the attack. Most effective to stop early stages (reconnaissance, delivery) before compromise occurs."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["MITRE ATT&CK", "Diamond Model", "Attack Framework", "Threat Modeling"]
    }
  },
  {
    "id": "d4-032",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "MITRE ATT&CK Framework",
    "back": {
      "level1": "Adversarial Tactics, Techniques, and Common Knowledge - Knowledge base of adversary behaviors",
      "level2": "Comprehensive matrix documenting real-world attacker tactics (strategic goals like Initial Access, Persistence, Privilege Escalation) and techniques (specific methods like Phishing, DLL Injection, Pass-the-Hash). Organized by platform (Windows, Linux, macOS, Cloud) and includes mitigations and detection methods for each technique.",
      "level3": "Exam tip: MITRE ATT&CK is about TTPs (Tactics, Techniques, Procedures). Know the major tactics: Initial Access, Execution, Persistence, Privilege Escalation, Defense Evasion, Credential Access, Discovery, Lateral Movement, Collection, Exfiltration, Impact. Unlike Kill Chain (linear), ATT&CK is a matrix showing multiple paths attackers can take. Used for: threat intelligence, detection engineering, red/blue team exercises. Questions test knowing what ATT&CK is (behavior knowledge base), how it differs from Kill Chain, and use cases. Common example techniques: T1566 (Phishing), T1078 (Valid Accounts)."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Cyber Kill Chain", "TTPs", "Threat Intelligence", "Red Team"]
    }
  },
  {
    "id": "d4-033",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Diamond Model",
    "back": {
      "level1": "Framework with four core features: Adversary, Capability, Infrastructure, Victim",
      "level2": "Intrusion analysis model showing relationships between four elements forming a diamond: Adversary (who), Capability (what tools/malware), Infrastructure (how - C2 servers, domains), and Victim (target). Each edge represents a relationship. Model helps analysts pivot between elements during investigation (if you know infrastructure, you can find other victims or capabilities used).",
      "level3": "Exam tip: Diamond Model is for ANALYSIS and PIVOTING during investigations. The four points: Adversary (threat actor), Capability (tools/malware/exploits), Infrastructure (domains, IPs, email addresses, C2 servers), Victim (target org/person). Key concept: knowing one element helps discover others. Example: found malicious domain (infrastructure) → identify other victims using it → discover adversary's other infrastructure → find additional capabilities. Questions test understanding the four elements and how model aids investigation. Remember: this is for INVESTIGATION, not prevention."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Threat Intelligence", "Incident Analysis", "IOCs", "Pivoting"]
    }
  },
  {
    "id": "d4-034",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Indicators of Compromise (IOCs)",
    "back": {
      "level1": "Observable artifacts that indicate potential intrusion or malicious activity",
      "level2": "Technical evidence of compromise including: suspicious IP addresses, malicious domains, file hashes (MD5/SHA256), registry key changes, unusual network traffic patterns, unexpected processes, known malware signatures. IOCs are shared via threat intelligence feeds and used to detect and block threats across organizations.",
      "level3": "Exam tip: IOCs are EVIDENCE of compromise. Common types: IP addresses of C2 servers, malicious file hashes, domains hosting malware, email addresses used in phishing, suspicious registry keys, mutex names, file paths. Know that IOCs have shelf life - they change as attackers adapt. Shared via STIX/TAXII, threat feeds, ISACs. Consumed by SIEM, IDS/IPS, EDR, firewalls. Difference from IOAs (Indicators of Attack) - IOCs show compromise happened, IOAs show attack in progress. Questions test identifying IOC types and how they're used in detection."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["IOA", "Threat Intelligence", "STIX/TAXII", "Threat Feeds"]
    }
  },
  {
    "id": "d4-035",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Threat Hunting",
    "back": {
      "level1": "Proactive search for threats that evaded automated detection systems",
      "level2": "Hypothesis-driven investigation where analysts actively search for signs of compromise rather than waiting for alerts. Uses threat intelligence, behavioral analysis, and anomaly detection to find advanced threats. Starts with hypothesis (e.g., 'Attackers may be using PowerShell for persistence'), then hunts for evidence using logs, EDR data, and forensic tools.",
      "level3": "Exam tip: Threat hunting is PROACTIVE (vs reactive alert response). Key characteristics: hypothesis-driven (based on threat intelligence or anomaly observations), requires skilled analysts, uses multiple data sources (logs, EDR, network traffic), focuses on finding sophisticated threats that bypass automated defenses. Common techniques: searching for known TTPs, behavioral anomalies, IOC sweeps across environment. Know that hunting often discovers new detection rules to add to automated systems. Questions test distinguishing hunting (proactive, hypothesis-based) from monitoring (reactive, alert-based)."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Threat Intelligence", "SIEM", "EDR", "Proactive Security"]
    }
  },
  {
    "id": "d4-036",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Syslog",
    "back": {
      "level1": "Standard protocol for message logging with severity levels 0-7",
      "level2": "Protocol for transmitting log messages from network devices, servers, and applications to centralized log collectors. Uses UDP port 514 (or TCP 6514 for syslog-ng/rsyslog). Messages include timestamp, hostname, facility (source), severity, and message content. Severity levels: 0=Emergency, 1=Alert, 2=Critical, 3=Error, 4=Warning, 5=Notice, 6=Informational, 7=Debug.",
      "level3": "Exam tip: MEMORIZE severity levels 0-7. Emergency (0) is most severe (system unusable), Debug (7) is least severe. Common question: which severity for specific events. Know that Syslog uses UDP by default (unreliable but fast), can use TCP for reliability. Facilities identify message source (kern, user, mail, daemon, auth, local0-7). Messages sent to SIEM or log management system. Configuration typically done in /etc/syslog.conf or /etc/rsyslog.conf. Questions test severity levels, protocol details (UDP 514), and use cases for centralized logging."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Log Management", "SIEM", "rsyslog", "Centralized Logging"]
    }
  },
  {
    "id": "d4-037",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Windows Event Logs",
    "back": {
      "level1": "Windows logging system capturing security, system, and application events with Event IDs",
      "level2": "Three main logs: Security (authentication, access, policy changes), System (hardware, drivers, services), Application (software events). Events identified by Event ID - unique number for each event type. Viewed via Event Viewer (eventvwr.msc). Critical for forensics and security monitoring. Can forward to SIEM using WEF (Windows Event Forwarding) or agents.",
      "level3": "Exam tip: Know key Event IDs: 4624 (successful logon), 4625 (failed logon), 4720 (account created), 4722 (account enabled), 4724 (password reset), 4728 (user added to security group), 4732 (user added to local group), 4740 (account lockout), 4768 (Kerberos TGT requested), 4769 (Kerberos service ticket), 7045 (service installed). Security log requires audit policy enabled. Questions test knowing which Event ID corresponds to specific actions (failed login=4625, successful=4624). Understand log clearing (Event ID 1102) is indicator of tampering."
    },
    "metadata": {
      "difficulty": "hard",
      "commonlyTested": true,
      "relatedTerms": ["Event Viewer", "Log Analysis", "SIEM", "Forensics"]
    }
  },
  {
    "id": "d4-038",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Log Correlation",
    "back": {
      "level1": "Analyzing relationships between events from multiple log sources to identify patterns",
      "level2": "Process of combining and analyzing logs from different sources (firewall, IDS, authentication servers, applications) to detect complex attack patterns that wouldn't be visible in single log source. SIEM performs correlation using rules and machine learning to connect related events and generate meaningful alerts from noisy data.",
      "level3": "Exam tip: Correlation is about finding RELATIONSHIPS across logs. Example: firewall shows port scan from IP 1.2.3.4, then authentication log shows failed logins from same IP, then IDS detects exploit attempt - correlation connects these to identify coordinated attack. Differs from aggregation (just collecting logs centrally). Correlation rules define patterns like: if failed logins > 5 in 1 minute from same IP = brute force attempt. Advanced correlation uses time windows, thresholds, and sequences. Questions test understanding correlation value and distinguishing from simple log collection."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SIEM", "Log Analysis", "Event Correlation", "Pattern Detection"]
    }
  },
  {
    "id": "d4-039",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Packet Capture (PCAP)",
    "back": {
      "level1": "Recording full network packets for detailed analysis",
      "level2": "Capturing complete network traffic including headers and payloads, stored in pcap format. Provides deepest visibility - can reconstruct TCP sessions, extract files, analyze application data. Uses tools like tcpdump, Wireshark, tshark. Requires significant storage (captures everything) and processing power. Used for forensics, troubleshooting, and deep threat analysis.",
      "level3": "Exam tip: PCAP captures EVERYTHING vs NetFlow which captures metadata only. PCAP advantages: full packet data, can analyze payload, reconstruct sessions, extract files. Disadvantages: massive storage requirements, privacy concerns (captures sensitive data), performance impact, not scalable to large networks. Know when to use: forensic investigations, malware analysis, deep troubleshooting. Filter expressions (BPF - Berkeley Packet Filter) reduce captured data. Questions test PCAP vs NetFlow trade-offs and knowing PCAP provides complete traffic details."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Wireshark", "tcpdump", "NetFlow", "Network Forensics"]
    }
  },
  {
    "id": "d4-040",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "tool",
    "front": "Wireshark",
    "back": {
      "level1": "Open-source network protocol analyzer for capturing and analyzing packets",
      "level2": "GUI-based packet capture and analysis tool that can capture live traffic or open pcap files. Provides detailed decode of hundreds of protocols, allows filtering, following TCP streams, extracting files, and statistical analysis. Essential tool for network troubleshooting, security analysis, and forensic investigations.",
      "level3": "Exam tip: Wireshark is the most common packet analyzer. Key features: capture filters (limit what's captured - uses BPF syntax), display filters (filter after capture - uses Wireshark syntax), follow TCP stream (reconstruct conversations), export objects (extract files from HTTP), protocol hierarchy (see traffic breakdown). Know common filters: ip.addr==x.x.x.x (specific IP), tcp.port==80 (HTTP traffic), http.request (HTTP requests only). Questions test knowing Wireshark is for packet analysis and basic use cases. Command-line equivalent: tshark."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["tcpdump", "Packet Analysis", "PCAP", "Network Forensics"]
    }
  },
  {
    "id": "d4-041",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Security Operations Center (SOC)",
    "back": {
      "level1": "Centralized unit that monitors, detects, analyzes, and responds to security incidents",
      "level2": "Team of security analysts working in shifts to provide 24/7 monitoring of organizational security. Uses SIEM, IDS/IPS, EDR, and other tools to detect threats. Structured in tiers: Tier 1 (triage, initial investigation), Tier 2 (deep analysis, incident response), Tier 3 (threat hunting, advanced analysis, tool development). Handles alerts, incidents, and continuous monitoring.",
      "level3": "Exam tip: Understand SOC tier structure. Tier 1 analysts (junior) - monitor dashboards, triage alerts, escalate suspicious activity, handle most volume. Tier 2 (senior) - deep investigation, incident response, correlation analysis, handle escalations. Tier 3 (expert) - threat hunting, malware analysis, tool development, handle complex incidents. Some orgs add Tier 0 (automated triage) and SOC Manager. Know SOC metrics: MTTD (Mean Time to Detect), MTTR (Mean Time to Respond), alert volume, false positive rate. Questions test understanding tier responsibilities and SOC role in security operations."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SIEM", "Incident Response", "MTTD", "MTTR", "Alert Triage"]
    }
  },
  {
    "id": "d4-042",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "MTTD and MTTR",
    "back": {
      "level1": "Mean Time to Detect and Mean Time to Respond - Key security metrics",
      "level2": "MTTD measures average time from when security incident occurs until it's detected. MTTR measures average time from detection until incident is fully resolved/remediated. Both are critical metrics for measuring security program effectiveness. Lower values indicate better security operations. Industry average MTTD is weeks-months for sophisticated attacks.",
      "level3": "Exam tip: Know both metrics and what they measure. MTTD: compromise → detection (shorter is better). MTTR: detection → resolution (shorter is better). Example: malware infected system on Jan 1, detected Jan 15 (MTTD=14 days), resolved Jan 16 (MTTR=1 day). Improving MTTD: better monitoring, threat hunting, behavioral detection, log correlation. Improving MTTR: automation (SOAR), playbooks, skilled analysts, clear procedures. Questions test understanding what each metric measures and how to improve them. Related: dwell time (how long attacker stays undetected - industry average ~200 days)."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SOC", "Metrics", "KPI", "Incident Response", "Dwell Time"]
    }
  },
  {
    "id": "d4-043",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Alert Tuning and Fatigue",
    "back": {
      "level1": "Optimizing detection rules to reduce false positives; burnout from excessive alerts",
      "level2": "Alert tuning: adjusting SIEM rules, IDS signatures, and detection thresholds to minimize false positives while maintaining detection capability. Alert fatigue: condition where analysts become desensitized due to overwhelming alert volume, causing them to miss real threats. SOCs typically face thousands of alerts daily, most being false positives.",
      "level3": "Exam tip: Alert fatigue is major SOC problem leading to missed threats. Caused by: too many alerts, high false positive rate, alert duplication, poor prioritization. Solutions: tune detection rules, whitelist known-good activity, automate triage, prioritize by risk, implement SOAR for automated response. Know the balance: too sensitive = alert fatigue, too lenient = missed attacks. Questions test understanding alert fatigue causes and mitigations. Remember: quality over quantity - better to have fewer accurate alerts than thousands of false positives."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["SOC", "SIEM", "False Positives", "Detection Engineering"]
    }
  },
  {
    "id": "d4-044",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "tool",
    "front": "Nessus",
    "back": {
      "level1": "Popular commercial vulnerability scanner by Tenable",
      "level2": "Comprehensive vulnerability scanning tool that identifies security weaknesses, misconfigurations, missing patches, and compliance violations. Supports credentialed and non-credentialed scans across networks, systems, applications, and cloud. Provides detailed remediation guidance and integrates with patch management systems. Uses plugin-based architecture with frequent updates.",
      "level3": "Exam tip: Nessus is one of most common vulnerability scanners (along with Qualys, Rapid7, OpenVAS). Key features: credentialed scanning for deeper assessment, compliance checks (PCI DSS, HIPAA), plugin-based (regularly updated vulnerability checks), risk-based prioritization. Know it performs active scanning (sends probes to targets) and can identify: missing patches, misconfigurations, weak passwords, outdated software. Nessus Professional for small deployments, Tenable.sc (Security Center) for enterprise. Questions test knowing Nessus is vulnerability scanner and its capabilities."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Vulnerability Scanning", "Qualys", "OpenVAS", "CVSS"]
    }
  },
  {
    "id": "d4-045",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "tool",
    "front": "FTK (Forensic Toolkit)",
    "back": {
      "level1": "Commercial forensic software for analyzing digital evidence",
      "level2": "Comprehensive forensic suite by AccessData/Exterro including FTK Imager (acquisition) and FTK (analysis). Features: disk imaging, file recovery, registry analysis, email analysis, password cracking, timeline creation, keyword searching. Widely used by law enforcement and corporate investigators. Known for fast processing of large datasets.",
      "level3": "Exam tip: FTK is popular commercial forensic tool. Two components: FTK Imager (free tool for creating forensic images and previewing evidence) and FTK (full analysis platform). Key capabilities: process large volumes of data quickly, recover deleted files, crack passwords, analyze registry, search across multiple evidence sources, create detailed reports. Uses database for indexing which enables fast searching. Alternative tools: EnCase, X-Ways, Autopsy (free). Questions test knowing FTK is for forensic analysis and evidence examination."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["EnCase", "Autopsy", "Digital Forensics", "Evidence Analysis"]
    }
  },
  {
    "id": "d4-046",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "File Carving",
    "back": {
      "level1": "Recovering files from unallocated disk space without file system metadata",
      "level2": "Forensic technique for extracting files from disk images by identifying file signatures (magic numbers) and structures rather than relying on file system. Can recover deleted files even after file system metadata is gone. Tools analyze raw disk data looking for file headers (JPEG starts with FFD8, PDF with %PDF) and footers to reconstruct files.",
      "level3": "Exam tip: File carving recovers files when file system metadata is deleted/corrupted. Works by: scanning disk sectors for file signatures (magic numbers - known byte patterns indicating file type), extracting data between header and footer, reconstructing file. Common with: deleted files, formatted drives, corrupted file systems. Tools: Foremost, Scalpel, PhotoRec. Know that carved files lose original filename and timestamps (metadata gone). Useful for recovering evidence after suspect 'deletes' files or formats drive. Questions test understanding file carving recovers deleted/hidden files using signatures."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Deleted Files", "Data Recovery", "Magic Numbers"]
    }
  },
  {
    "id": "d4-047",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Timeline Analysis",
    "back": {
      "level1": "Reconstructing sequence of events by analyzing timestamps from multiple sources",
      "level2": "Forensic technique creating chronological view of system activity by correlating timestamps from: file system (MAC times - Modified, Accessed, Changed), logs, registry, browser history, application data. Helps answer: What happened? When? In what order? Timeline tools (log2timeline, plaso) automate collection and correlation of timestamps.",
      "level3": "Exam tip: Timeline analysis reconstructs event sequence during investigation. Key timestamp sources: file MAC times (Modified, Accessed, Changed/Created), Windows registry, event logs, browser history, application logs, email headers, network connection logs. Know that timestamps can be in different time zones and formats - must normalize. Tools: log2timeline/plaso (create super timelines), Autopsy, FTK. Used to: establish user activity sequence, prove data access, identify when malware executed. Questions test understanding timeline analysis correlates timestamps to reconstruct events."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "MAC Times", "log2timeline", "Event Reconstruction"]
    }
  },
  {
    "id": "d4-048",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Threat Intelligence Sources",
    "back": {
      "level1": "OSINT, Commercial Feeds, ISACs/ISAOs, Dark Web, Internal Sources",
      "level2": "Open Source Intelligence (OSINT): publicly available data from blogs, news, security researchers. Commercial Feeds: paid services providing IOCs and analysis (CrowdStrike, Recorded Future). ISACs/ISAOs: Information Sharing and Analysis Centers for specific industries. Dark Web Monitoring: monitoring criminal forums and markets. Internal: organization's own incident data, honeypot logs.",
      "level3": "Exam tip: Know threat intelligence source types. OSINT advantages: free, timely; disadvantages: not tailored, lower fidelity. Commercial feeds: high quality, curated, expensive. ISACs: industry-specific, peer sharing (FS-ISAC for financial, H-ISAC for healthcare). Know that intelligence should be: timely (current threats), relevant (applies to your environment), actionable (can act on it). Tactical intelligence: IOCs for immediate defense. Operational: adversary TTPs. Strategic: trends and risk landscape. Questions test source types and when to use each."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["OSINT", "ISAC", "IOC", "Threat Feeds", "Dark Web"]
    }
  },
  {
    "id": "d4-049",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Incident Categories",
    "back": {
      "level1": "DDoS, Ransomware, Data Breach, Insider Threat, Phishing, Malware, Unauthorized Access",
      "level2": "Common incident types requiring different response approaches. DDoS: service disruption. Ransomware: encryption with extortion. Data Breach: unauthorized data access/exfiltration. Insider Threat: malicious employee. Phishing: social engineering. Malware: virus/worm/trojan infection. Unauthorized Access: compromised credentials or exploitation. Each category has specific containment and recovery procedures.",
      "level3": "Exam tip: Know different incident types and response priorities. Ransomware: isolate immediately, check backups, DON'T pay (usually). Data breach: contain, identify scope, legal notification, forensics. DDoS: traffic filtering, rate limiting, CDN/scrubbing service. Insider threat: preserve evidence, revoke access, legal hold. Phishing: user education, email filtering, credential reset. Questions test identifying incident type from scenario and appropriate initial response. Remember: containment approach varies - ransomware needs aggressive isolation, data breach needs evidence preservation."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Ransomware", "Data Breach", "DDoS"]
    }
  },
  {
    "id": "d4-050",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Root Cause Analysis",
    "back": {
      "level1": "Identifying the underlying reason for security incident occurrence",
      "level2": "Investigative process determining how attacker initially compromised the environment. Goes beyond immediate cause (malware execution) to find root cause (unpatched vulnerability, weak password, successful phish). Uses techniques like 5 Whys, fishbone diagrams, and timeline analysis. Essential for preventing recurrence and improving security.",
      "level3": "Exam tip: Root cause analysis finds the ORIGIN of compromise, not just symptoms. Example: symptom=ransomware encrypted files, immediate cause=malware executed, root cause=user clicked phishing email → unpatched email client vulnerability → lack of security awareness training. Process: gather evidence, reconstruct attack timeline, identify entry point, determine why defenses failed, recommend fixes. Part of lessons learned phase. Know common root causes: unpatched vulnerabilities, weak passwords, lack of MFA, insufficient monitoring, user error. Questions test understanding RCA finds origin to prevent recurrence."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "Lessons Learned", "5 Whys", "Post-Incident"]
    }
  },
  {
    "id": "d4-051",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Isolation vs Segmentation",
    "back": {
      "level1": "Isolation: complete disconnect; Segmentation: controlled separation into network zones",
      "level2": "Isolation: completely disconnecting compromised system from all networks (pull network cable, disable NIC, air gap). Prevents spread but stops evidence collection and monitoring. Segmentation: moving system to isolated VLAN/network segment with monitoring, allowing controlled analysis while preventing lateral movement. Maintains some connectivity for investigation.",
      "level3": "Exam tip: Key distinction for incident response containment. Isolation = COMPLETE disconnect (most aggressive, stops all communication, use for ransomware or rapidly spreading malware). Segmentation = CONTROLLED separation (allows monitoring and analysis, use when need to observe attacker or gather evidence). Questions often ask which is appropriate for scenarios. Isolation pros: absolute containment. Cons: can't monitor attacker, lose volatile evidence if shutdown. Segmentation pros: continued monitoring, preserve evidence. Cons: still some risk if misconfigured. Choose based on threat severity and investigation needs."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Containment", "Incident Response", "Network Segmentation", "Air Gap"]
    }
  },
  {
    "id": "d4-052",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Live vs Dead Acquisition",
    "back": {
      "level1": "Live: capturing evidence from running system; Dead: imaging powered-off system",
      "level2": "Live acquisition: collecting evidence while system is running - captures volatile data (RAM, active connections, running processes) but risks altering evidence. Dead acquisition: powering off system then imaging - preserves disk state but loses volatile data. Choice depends on whether volatile evidence is critical and risk of evidence modification.",
      "level3": "Exam tip: Know trade-offs. Live acquisition: captures RAM, running processes, network connections, encryption keys (needed for encrypted drives). Risks: running acquisition tools modifies system state, suspect could detect. Used when: volatile evidence needed, encrypted drives (keys in memory), long-running investigations. Dead acquisition: safer for evidence integrity, standard for most cases. Loses: memory contents, active connections, processes. Best practice: if doing live, capture volatile data first (order of volatility), then do memory dump, then disk image. Questions test when each is appropriate and understanding trade-offs."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Volatile Data", "Evidence Collection", "Memory Dump"]
    }
  },
  {
    "id": "d4-053",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Hashing Algorithms in Forensics",
    "back": {
      "level1": "MD5, SHA-1, SHA-256 used to verify evidence integrity",
      "level2": "Cryptographic hash functions create unique fingerprint of files or disk images. Hashing original evidence and forensic copy allows verification that copy is exact duplicate and hasn't been modified. Any change to evidence results in different hash. Hash values documented in chain of custody. SHA-256 preferred over MD5/SHA-1 due to better collision resistance.",
      "level3": "Exam tip: Hashing proves evidence INTEGRITY. Process: hash original evidence, create forensic copy, hash copy, compare hashes - if match, copy is exact. MD5 (128-bit): fast but has known collision vulnerabilities, still widely used. SHA-1 (160-bit): better than MD5 but also has weaknesses. SHA-256 (256-bit): current best practice, most secure. Hashes included in forensic reports and chain of custody documentation. Know that even tiny change (single bit) produces completely different hash. Questions test understanding hashing verifies evidence wasn't modified and knowing which algorithms are used."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "MD5", "SHA-256", "Evidence Integrity", "Chain of Custody"]
    }
  },
  {
    "id": "d4-054",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Agent-based vs Agentless Scanning",
    "back": {
      "level1": "Agent-based: software installed on endpoints; Agentless: network-based scanning",
      "level2": "Agent-based: vulnerability scanner installs agent software on each system providing continuous assessment, detailed visibility, works from inside firewall. Agentless: scanner probes systems over network without installing software, easier deployment but less detailed, requires network access and credentials for deep scans.",
      "level3": "Exam tip: Know trade-offs. Agent-based advantages: continuous monitoring, works offline/mobile devices, detailed system info, no network scanning impact. Disadvantages: deployment/maintenance overhead, agent conflicts, resource usage. Agentless advantages: no deployment, no endpoint resources, scans any network-accessible device. Disadvantages: point-in-time only, network overhead, may require credentials, limited mobile/remote coverage. Questions test when to use each: use agent-based for laptops/remote workers, agentless for network devices/servers/OT. Modern approach: hybrid using both."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Vulnerability Scanning", "Endpoint Security", "Assessment"]
    }
  },
  {
    "id": "d4-055",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Tabletop Exercise",
    "back": {
      "level1": "Discussion-based incident response simulation without actual system changes",
      "level2": "IR testing method where team walks through incident scenario in meeting format. Facilitator presents scenario (ransomware attack, data breach), team discusses response steps, identifies gaps in procedures, clarifies roles. Low-cost, low-risk way to test and improve IR plans. Identifies procedural gaps, communication issues, and training needs without actual incident.",
      "level3": "Exam tip: Tabletop is DISCUSSION-based (vs hands-on drill). Advantages: low cost, no downtime, safe environment, tests procedures and communication. Process: prepare scenario, assemble IR team, facilitator presents situation and injects new details, team discusses response actions, document lessons learned. Know the exercise levels: tabletop (discussion), walkthrough (step through procedures), functional (simulate without full activation), full-scale (actual drill). Questions test knowing tabletop is discussion-based IR practice and its benefits for team preparation without disruption."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Incident Response", "IR Testing", "Business Continuity", "Exercise"]
    }
  },
  {
    "id": "d4-056",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Data Loss Prevention (DLP)",
    "back": {
      "level1": "Technology that detects and prevents unauthorized data exfiltration",
      "level2": "Security solution monitoring data in use, in motion, and at rest to prevent unauthorized transmission. Uses content inspection (keywords, patterns, file types), contextual analysis (who, what, when, where), and data classification tags. Can detect sensitive data (SSN, credit cards, PII) being emailed, uploaded, copied to USB, or sent to unauthorized destinations.",
      "level3": "Exam tip: DLP has three protection points: Data at Rest (on servers/endpoints - scans and tags), Data in Motion (network/email - blocks transmission), Data in Use (endpoint activity - prevents copy/paste, screenshot, print). Detection methods: pattern matching (regex for SSN/CC), fingerprinting (exact file matches), classification tags (documents marked confidential). Actions: alert only, block, quarantine, encrypt. Know that DLP requires: data classification, tuning to avoid false positives, user training. Questions test DLP use cases (prevent data exfiltration) and understanding three protection states."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Data Exfiltration", "Data Classification", "Insider Threat", "Content Filtering"]
    }
  },
  {
    "id": "d4-057",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "File Integrity Monitoring (FIM)",
    "back": {
      "level1": "Detecting unauthorized changes to critical files by monitoring file hashes",
      "level2": "Security control that creates baseline hashes of critical system files, configurations, and binaries, then continuously monitors for changes. Alerts when files are modified, added, or deleted. Detects: malware installation, unauthorized configuration changes, rootkits, compliance violations. Tools include Tripwire, AIDE (Advanced Intrusion Detection Environment), OSSEC.",
      "level3": "Exam tip: FIM detects UNAUTHORIZED CHANGES using hashing. Process: create baseline (hash all critical files), store hashes securely, periodically re-hash files, alert on differences. Monitors: OS files, application binaries, configuration files, critical data. Know that changes trigger alerts requiring investigation - could be legitimate patching/updates or malicious modification. PCI DSS requires FIM for critical files. Tools: Tripwire (commercial), AIDE (Linux open source), Windows File Checksum Integrity Verifier. Questions test understanding FIM detects file tampering and knowing it uses hashing for comparison."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Tripwire", "AIDE", "Integrity", "Change Detection", "PCI DSS"]
    }
  },
  {
    "id": "d4-058",
    "domain": 4,
    "section": "Digital Forensics",
    "type": "concept",
    "front": "Metadata and MAC Times",
    "back": {
      "level1": "File metadata includes MAC times: Modified, Accessed, Changed/Created timestamps",
      "level2": "Metadata is data about data - file properties beyond content. MAC times are filesystem timestamps: Modified (content changed), Accessed (file opened/read), Changed (metadata/permissions changed - Linux) or Created (file creation - Windows). Also includes: file size, owner, permissions, EXIF data for images. Critical for timeline analysis and establishing user activity.",
      "level3": "Exam tip: MAC times are crucial forensic evidence. Modified: when file content last changed. Accessed: when file last opened (unreliable - can be disabled for performance, updated on reads). Changed/Created: Linux uses Changed (inode modification), Windows uses Created (file creation time). Know that: MAC times can be manipulated by attackers (timestomping), EXIF data in images contains camera info/GPS/timestamps, metadata survives file copying (though timestamps may change). Questions test understanding MAC times meaning and their forensic value for establishing file activity timeline."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Forensics", "Timeline Analysis", "EXIF", "Timestamps", "Timestomping"]
    }
  },
  {
    "id": "d4-059",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "standard",
    "front": "OWASP Top 10",
    "back": {
      "level1": "Open Web Application Security Project's list of most critical web application security risks",
      "level2": "Regularly updated list of most common and critical web application vulnerabilities. 2021 Top 10 includes: Broken Access Control, Cryptographic Failures, Injection, Insecure Design, Security Misconfiguration, Vulnerable and Outdated Components, Identification and Authentication Failures, Software and Data Integrity Failures, Security Logging and Monitoring Failures, Server-Side Request Forgery (SSRF).",
      "level3": "Exam tip: Know top risks and examples. Broken Access Control (privilege escalation, IDOR). Injection (SQL injection, command injection). Authentication Failures (weak passwords, session management). Security Misconfiguration (default credentials, unnecessary features). Know that OWASP also publishes: API Security Top 10, Mobile Top 10, guidance documents. Used for: security training, code review checklists, pentesting scope, risk awareness. Questions test knowing OWASP Top 10 is web application security reference and recognizing common vulnerabilities like SQL injection, XSS (Cross-Site Scripting), broken authentication."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Web Security", "SQL Injection", "XSS", "Application Security"]
    }
  },
  {
    "id": "d4-060",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Configuration Scanning",
    "back": {
      "level1": "Automated assessment of system settings against security baselines",
      "level2": "Scanning systems to verify they comply with security configuration standards like CIS Benchmarks or DISA STIGs. Checks: unnecessary services disabled, secure password policies, proper permissions, encryption enabled, logging configured, secure protocols. SCAP scanners automate this process. Identifies configuration drift from approved baselines.",
      "level3": "Exam tip: Configuration scanning checks SETTINGS vs vulnerability scanning checks PATCHES. Uses baselines: CIS Benchmarks (consensus community standards), DISA STIGs (DoD), vendor hardening guides (Microsoft Security Baseline), regulatory requirements (PCI DSS). SCAP (Security Content Automation Protocol) enables automated compliance checking. Findings: non-compliant settings requiring remediation. Know that configuration weaknesses are common attack vectors - default passwords, unnecessary services, weak encryption. Questions test distinguishing configuration scanning (settings/compliance) from vulnerability scanning (CVE/patches)."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["CIS Benchmarks", "DISA STIG", "SCAP", "Compliance", "Hardening"]
    }
  },
  {
    "id": "d4-061",
    "domain": 4,
    "section": "Incident Response",
    "type": "concept",
    "front": "Evidence Preservation",
    "back": {
      "level1": "Protecting digital evidence from alteration, damage, or destruction",
      "level2": "Ensuring evidence remains in original state for legal admissibility. Includes: write blockers during acquisition, hashing to prove integrity, proper storage in tamper-evident containers, access logging, environmental controls (temperature, humidity for magnetic media), chain of custody documentation. Any evidence modification or improper handling risks inadmissibility in court.",
      "level3": "Exam tip: Evidence preservation is critical for legal proceedings. Key principles: minimize changes to original (use write blockers, forensic software), document everything (chain of custody), hash for integrity verification, store securely (locked evidence room, access logs), maintain environmental controls. Know that even viewing files can modify access timestamps - why forensic copies are used for analysis. Questions test understanding preservation requirements: use of write blockers, hashing, chain of custody, analyzing copies not originals. Remember: improperly handled evidence may be excluded from legal proceedings."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Chain of Custody", "Write Blocker", "Forensics", "Legal Hold"]
    }
  },
  {
    "id": "d4-062",
    "domain": 4,
    "section": "Security Monitoring",
    "type": "concept",
    "front": "Honeypot and Honeynet",
    "back": {
      "level1": "Honeypot: decoy system; Honeynet: network of honeypots designed to attract and study attackers",
      "level2": "Honeypot is intentionally vulnerable system with no legitimate purpose, deployed to detect attacks and gather intelligence. Attackers waste time on decoys while revealing TTPs. Honeynet is network of multiple honeypots simulating enterprise environment. Low-interaction honeypots emulate services. High-interaction are full systems. Used for research, detection, and deception.",
      "level3": "Exam tip: Honeypots serve three purposes: DETECTION (alerts when accessed - no legitimate users), DEFLECTION (waste attacker time), INTELLIGENCE (study attacker methods). Low-interaction: emulated services, safer, less realistic. High-interaction: real systems, more dangerous (can be used to attack others), more realistic. Placement: outside firewall (Internet-facing) or inside (detect insider/lateral movement). Legal/ethical considerations: honey tokens (fake data), deception ethics, liability if honeypot used in attacks. Questions test honeypot purpose (detect/study attackers) and understanding they're decoys with no production purpose."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["Deception", "Threat Intelligence", "Intrusion Detection", "Honey Token"]
    }
  },
  {
    "id": "d4-063",
    "domain": 4,
    "section": "Security Automation",
    "type": "concept",
    "front": "Security Automation Scripts",
    "back": {
      "level1": "Using PowerShell, Python, Bash scripts to automate security tasks",
      "level2": "Scripting enables automation of repetitive security tasks: log analysis, IOC checking, user provisioning/deprovisioning, compliance scanning, incident response actions, report generation. PowerShell for Windows administration, Python for cross-platform security tools, Bash for Linux/Unix automation. Scripts can integrate with APIs of security tools for orchestration.",
      "level3": "Exam tip: Know common security scripting use cases. PowerShell: Active Directory management, Windows event log analysis, endpoint configuration. Python: parsing logs, IOC lookup, API integration, security tool development. Bash: Linux administration, log processing, cron jobs for scheduled tasks. Benefits: consistency, speed, reduced human error, scalability. Risks: improper scripts can cause damage, credential management in scripts, testing before production. Questions test understanding automation value for security operations and knowing which languages are commonly used (PowerShell/Python/Bash)."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": false,
      "relatedTerms": ["PowerShell", "Python", "Bash", "Automation", "Orchestration"]
    }
  },
  {
    "id": "d4-064",
    "domain": 4,
    "section": "Vulnerability Management",
    "type": "concept",
    "front": "Scan Frequency and Timing",
    "back": {
      "level1": "How often to perform vulnerability scans based on risk and change rate",
      "level2": "Scanning frequency depends on: system criticality, change rate, regulatory requirements, risk tolerance. Critical systems: continuous or daily. Standard systems: weekly or monthly. Timing considerations: scan during maintenance windows to avoid business impact, after patching to verify, when new threats emerge, before production deployment. Balance thoroughness with network/system impact.",
      "level3": "Exam tip: No one-size-fits-all schedule. Factors determining frequency: PCI DSS requires quarterly external and internal scans (after changes too), HIPAA suggests regular assessments, high-risk systems need more frequent scanning. Timing: credentialed scans impact performance so schedule off-peak, non-credentialed can run anytime. Continuous scanning with agents vs scheduled network scans. Post-patch scanning verifies remediation. Questions test understanding scanning frequency varies by risk/compliance requirements and knowing scan timing should minimize business impact."
    },
    "metadata": {
      "difficulty": "easy",
      "commonlyTested": true,
      "relatedTerms": ["Vulnerability Scanning", "PCI DSS", "Risk Management", "Compliance"]
    }
  },
  {
    "id": "d4-065",
    "domain": 4,
    "section": "Threat Intelligence",
    "type": "concept",
    "front": "Threat Feed Integration",
    "back": {
      "level1": "Automated consumption of threat intelligence IOCs into security tools",
      "level2": "Integrating external threat feeds (IP reputation, malicious domains, file hashes) into security infrastructure. SIEM, firewalls, IDS/IPS, EDR, and proxies consume feeds to automatically block or alert on known threats. Feeds delivered via STIX/TAXII, APIs, or simple lists. Requires tuning to avoid false positives and ensure relevance to environment.",
      "level3": "Exam tip: Threat feeds provide AUTOMATED threat intelligence. Sources: commercial (high quality, expensive), open source (free, variable quality), ISACs (industry-specific). Integration points: SIEM (correlate with events), firewall/IPS (block bad IPs), DNS (block malicious domains), proxy (block URLs), EDR (block file hashes). Challenges: feed quality varies, false positives, feeds can be large (performance impact), need context (why is IP bad?). Questions test understanding feeds provide automated IOC updates and knowing they integrate into multiple security tools for automated blocking/alerting."
    },
    "metadata": {
      "difficulty": "medium",
      "commonlyTested": true,
      "relatedTerms": ["IOC", "STIX/TAXII", "Threat Intelligence", "Automation"]
    }
  }
]
